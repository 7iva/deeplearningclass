{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mlp.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/chokkan/deeplearningclass/blob/master/mlp-binary.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "CX_9BuPB_hA-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Feedforward Neural Networks\n",
        "\n",
        "This Jupyter notebook explains various ways of implementing single-layer and multi-layer neural networks. The implementations are arranged by concrete (explicit) to abstract order so that one can understand the black-boxed process in deep learning frameworks.\n",
        "\n",
        "In order to focus on understanding the internals of training, this notebook uses a simple and classic example: *threshold logic units*.\n",
        "Supposing $x=0$ as *false* and $x=1$ as *true*, single-layer neural networks can realize logic units such as AND ($\\wedge$), OR ($\\vee$), NOT ($\\lnot$), and NAND ($|$). Multi-layer neural networks can realize logical compounds such as XOR.\n",
        "\n",
        "| $x_1$ | $x_2$ | AND | OR | NAND | XOR |\n",
        "| :---: |:-----:|:---:|:--:|:----:|:---:|\n",
        "| 0 | 0 | 0 | 0 | 1 | 0 |\n",
        "| 0 | 1 | 0 | 1 | 1 | 1 |\n",
        "| 1 | 0 | 0 | 1 | 1 | 1 |\n",
        "| 1 | 1 | 1 | 1 | 0 | 0 |\n"
      ]
    },
    {
      "metadata": {
        "id": "qxdFG8U2Net8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Using numpy"
      ]
    },
    {
      "metadata": {
        "id": "Ea5cM4JEsENr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Single-layer perceptron\n",
        "\n",
        "A single layer perceptron predicts a binary label $\\hat{y} \\in \\{0, 1\\}$ for a given input vector $\\boldsymbol{x} \\in \\mathbb{R}^d$ ($d$ presents the number of dimensions of inputs) by using the following formula,\n",
        "$$\n",
        "\\hat{y} = g(\\boldsymbol{w} \\cdot \\boldsymbol{x} + b) = g(w_1 x_1 + w_2 x_2 + ... + w_d x_d + b)\n",
        "$$\n",
        "\n",
        "Here, $\\boldsymbol{w} \\in \\mathbb{R}^d$ is a weight vector; $b \\in \\mathbb{R}$ is a bias weight; and $g(.)$ denotes a Heaviside step function (we assume $g(0)=0$).\n",
        "\n",
        "Let's train a NAND gate with two inputs ($d = 2$). More specifically, we want to find a weight vector $\\boldsymbol{w}$ and a bias weight $b$ of a single-layer perceptron that realizes the truth table of the NAND gate: $\\{0,1\\}^2 \\to \\{0,1\\}$.\n",
        "\n",
        "| $x_1$ | $x_2$ | $y$  |\n",
        "| :---: |:-----:|:----:|\n",
        "| 0 | 0 | 1|\n",
        "| 0 | 1 | 1|\n",
        "| 1 | 0 | 1|\n",
        "| 1 | 1 | 0|\n",
        "\n",
        "We convert the truth table into a training set consisting of all mappings of the NAND gate,\n",
        "$$\n",
        "\\boldsymbol{x}_1 = (0, 0), y_1 = 1 \\\\\n",
        "\\boldsymbol{x}_2 = (0, 1), y_2 = 1 \\\\\n",
        "\\boldsymbol{x}_3 = (1, 0), y_3 = 1 \\\\\n",
        "\\boldsymbol{x}_4 = (1, 1), y_4 = 0 \\\\\n",
        "$$\n",
        "\n",
        "In order to train a weight vector bias weight in a unified code, we include a bias term as an additional dimension to inputs. More concretely, we append $1$ to each input,\n",
        "$$\n",
        "\\boldsymbol{x}'_1 = (0, 0, 1), y_1 = 1 \\\\\n",
        "\\boldsymbol{x}'_2 = (0, 1, 1), y_2 = 1 \\\\\n",
        "\\boldsymbol{x}'_3 = (1, 0, 1), y_3 = 1 \\\\\n",
        "\\boldsymbol{x}'_4 = (1, 1, 1), y_4 = 0 \\\\\n",
        "$$\n",
        "\n",
        "Then, the formula of the single-layer perceptron becomes,\n",
        "$$\n",
        "\\hat{y} = g((w_1, w_2, w_3) \\cdot \\boldsymbol{x}') = g(w_1 x_1 + w_2 x_2 + w_3)\n",
        "$$\n",
        "Therefore, $w_1$ and $w_2$ present weights for $x_1$ and $x_2$, respectively, and $w_3$ does a bias weight.\n",
        "\n",
        "The code below implements Rosenblatt's perceptron algorithm with a fixed number of iterations (100 times). We use a constant learning rate 0.5 for simplicity.\n"
      ]
    },
    {
      "metadata": {
        "id": "2ygoUjQYrPoj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Training data for NAND.\n",
        "x = np.array([\n",
        "    [0, 0, 1], [0, 1, 1], [1, 0, 1], [1, 1, 1]\n",
        "    ])\n",
        "y = np.array([1, 1, 1, 0])\n",
        "w = np.array([0.0, 0.0, 0.0])\n",
        "\n",
        "eta = 0.5\n",
        "for t in range(100):\n",
        "    for i in range(len(y)):\n",
        "        y_pred = np.heaviside(np.dot(x[i], w), 0)\n",
        "        w += (y[i] - y_pred) * eta * x[i]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TYoeshu2rXdK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "7d751395-ee62-46f2-eee5-b81c3740fdea"
      },
      "cell_type": "code",
      "source": [
        "w"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-1. , -0.5,  1.5])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "metadata": {
        "id": "hOFgUFojraFA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "23b46605-d0a6-4120-ee3c-cf48edbb9fc6"
      },
      "cell_type": "code",
      "source": [
        "np.heaviside(np.dot(x, w), 0)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1., 1., 1., 0.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "metadata": {
        "id": "bl_ZAEguNzgU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Single-layer perceptron with mini-batch\n",
        "\n",
        "It is desireable to reduce the execusion run by the Python interpreter because it is relatively slow. The common technique to speed up a machine-learning code written in Python is to to execute computations in the matrix library (e.g., numpy).\n",
        "\n",
        "The single-layer perceptron makes predictions for four inputs,\n",
        "$$\n",
        "\\hat{y}_1 = g(\\boldsymbol{x}_1 \\cdot \\boldsymbol{w}) \\\\\n",
        "\\hat{y}_2 = g(\\boldsymbol{x}_2 \\cdot \\boldsymbol{w}) \\\\\n",
        "\\hat{y}_3 = g(\\boldsymbol{x}_3 \\cdot \\boldsymbol{w}) \\\\\n",
        "\\hat{y}_4 = g(\\boldsymbol{x}_4 \\cdot \\boldsymbol{w}) \\\\\n",
        "$$\n",
        "\n",
        "Here, we define $\\hat{Y} \\in \\mathbb{R}^{4 \\times 1}$ and $X \\in \\mathbb{R}^{4 \\times d}$ as,\n",
        "$$\n",
        "\\hat{Y} = \\begin{pmatrix} \n",
        "  \\hat{y}_1 \\\\ \n",
        "  \\hat{y}_2 \\\\ \n",
        "  \\hat{y}_3 \\\\ \n",
        "  \\hat{y}_4 \\\\ \n",
        "\\end{pmatrix},\n",
        "X = \\begin{pmatrix} \n",
        "  \\boldsymbol{x}_1 \\\\ \n",
        "  \\boldsymbol{x}_2 \\\\ \n",
        "  \\boldsymbol{x}_3 \\\\ \n",
        "  \\boldsymbol{x}_4 \\\\ \n",
        "\\end{pmatrix}\n",
        "$$\n",
        "\n",
        "Then, we can write the four predictions in one dot-product computation,\n",
        "$$\n",
        "\\hat{Y} = X \\cdot \\boldsymbol{w}\n",
        "$$\n",
        "\n",
        "The code below implements this idea. The function `np.heaviside()` yields a vector corresponding to the four predictions, applying the step function for every element of the argument.\n",
        "\n",
        "This technique is frequently used in mini-batch training."
      ]
    },
    {
      "metadata": {
        "id": "2fK-_WimtPwb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Training data for NAND.\n",
        "x = np.array([\n",
        "    [0, 0, 1], [0, 1, 1], [1, 0, 1], [1, 1, 1]\n",
        "    ])\n",
        "y = np.array([1, 1, 1, 0])\n",
        "w = np.array([0.0, 0.0, 0.0])\n",
        "\n",
        "eta = 0.5\n",
        "for t in range(100):\n",
        "    y_pred = np.heaviside(np.dot(x, w), 0)\n",
        "    w += np.dot((y - y_pred), x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_x4p1BldtQ-K",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "7ef5b1c8-06d4-4820-a10c-89595bf2405a"
      },
      "cell_type": "code",
      "source": [
        "w"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-1., -1.,  2.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "metadata": {
        "id": "E-P2RpWrtVyf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "68b93059-14d6-43ab-f2d6-39229da83b2f"
      },
      "cell_type": "code",
      "source": [
        "np.heaviside(np.dot(x, w), 0)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1., 1., 1., 0.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "metadata": {
        "id": "nkxBcCSTtDvm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Stochastic gradient descent (SGD) with mini-batch"
      ]
    },
    {
      "metadata": {
        "id": "bltpfNRctjV5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def sigmoid(v):\n",
        "    return 1.0 / (1 + np.exp(-v))\n",
        "\n",
        "# Training data for NAND.\n",
        "x = np.array([\n",
        "    [0, 0, 1], [0, 1, 1], [1, 0, 1], [1, 1, 1]\n",
        "    ])\n",
        "y = np.array([1, 1, 1, 0])\n",
        "w = np.array([0.0, 0.0, 0.0])\n",
        "\n",
        "eta = 0.5\n",
        "for t in range(100):\n",
        "    y_pred = sigmoid(np.dot(x, w))\n",
        "    w -= np.dot((y_pred - y), x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9bASDMfhtm-I",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "fa764266-72dc-4bc8-d2a5-226e9565dbc3"
      },
      "cell_type": "code",
      "source": [
        "w"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-5.59504346, -5.59504346,  8.57206068])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "metadata": {
        "id": "-69r0c4KtqlW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "6573433e-bf1a-49b3-97d5-7cfd0d8ed848"
      },
      "cell_type": "code",
      "source": [
        "sigmoid(np.dot(x, w))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.99981071, 0.95152498, 0.95152498, 0.06798725])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "metadata": {
        "id": "ELUeNFRFuJv3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Automatic differentiation"
      ]
    },
    {
      "metadata": {
        "id": "aB0DwVOyuXP_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Using autograd\n",
        "\n",
        "Installing [autograd](https://github.com/HIPS/autograd) (do this once)."
      ]
    },
    {
      "metadata": {
        "id": "kpt7DJ7a-qov",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "outputId": "2861b15a-106a-43b5-fb91-b9dd614b0536"
      },
      "cell_type": "code",
      "source": [
        "!pip install autograd"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting autograd\n",
            "  Downloading https://files.pythonhosted.org/packages/08/7a/1ccee2a929d806ba3dbe632a196ad6a3f1423d6e261ae887e5fef2011420/autograd-1.2.tar.gz\n",
            "Requirement already satisfied: numpy>=1.12 in /usr/local/lib/python3.6/dist-packages (from autograd) (1.14.5)\n",
            "Requirement already satisfied: future>=0.15.2 in /usr/local/lib/python3.6/dist-packages (from autograd) (0.16.0)\n",
            "Building wheels for collected packages: autograd\n",
            "  Running setup.py bdist_wheel for autograd ... \u001b[?25l-\b \b\\\b \bdone\n",
            "\u001b[?25h  Stored in directory: /content/.cache/pip/wheels/72/6f/c2/40f130cca2c91f31d354bf72de282922479c09ce0b7853c4c5\n",
            "Successfully built autograd\n",
            "Installing collected packages: autograd\n",
            "Successfully installed autograd-1.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "DyC9iOFO-1cd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "346d5e82-a6ba-448d-eb89-4e6e42cd43f0"
      },
      "cell_type": "code",
      "source": [
        "import autograd\n",
        "import autograd.numpy as np\n",
        "\n",
        "def loss(w, x):\n",
        "    return -np.log(1.0 / (1 + np.exp(-np.dot(x, w))))\n",
        "\n",
        "x = np.array([1, 1, 1])\n",
        "w = np.array([1.0, 1.0, -1.5])\n",
        "\n",
        "grad_loss = autograd.grad(loss)\n",
        "print(loss(w, x))\n",
        "print(grad_loss(w, x))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.47407698418010663\n",
            "[-0.37754067 -0.37754067 -0.37754067]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "59D2aARTuQDL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Using pytorch"
      ]
    },
    {
      "metadata": {
        "id": "VOoZXvXP-6b2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        },
        "outputId": "6449c654-89d4-4b33-948c-514478151e83"
      },
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torch\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/69/43/380514bd9663f1bf708abeb359b8b48d3fabb1c8e95bb3427a980a064c57/torch-0.4.0-cp36-cp36m-manylinux1_x86_64.whl (484.0MB)\n",
            "\u001b[K    100% |████████████████████████████████| 484.0MB 23kB/s \n",
            "tcmalloc: large alloc 1073750016 bytes == 0x5bf78000 @  0x7fe8c0ede1c4 0x46d6a4 0x5fcbcc 0x4c494d 0x54f3c4 0x553aaf 0x54e4c8 0x54f4f6 0x553aaf 0x54efc1 0x54f24d 0x553aaf 0x54efc1 0x54f24d 0x553aaf 0x54efc1 0x54f24d 0x551ee0 0x54e4c8 0x54f4f6 0x553aaf 0x54efc1 0x54f24d 0x551ee0 0x54efc1 0x54f24d 0x551ee0 0x54e4c8 0x54f4f6 0x553aaf 0x54e4c8\n",
            "\u001b[?25hCollecting torchvision\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ca/0d/f00b2885711e08bd71242ebe7b96561e6f6d01fdb4b9dcf4d37e2e13c5e1/torchvision-0.2.1-py2.py3-none-any.whl (54kB)\n",
            "\u001b[K    100% |████████████████████████████████| 61kB 16.8MB/s \n",
            "\u001b[?25hCollecting pillow>=4.1.1 (from torchvision)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/24/f53ff6b61b3d728b90934bddb4f03f8ab584a7f49299bf3bde56e2952612/Pillow-5.2.0-cp36-cp36m-manylinux1_x86_64.whl (2.0MB)\n",
            "\u001b[K    100% |████████████████████████████████| 2.0MB 15.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.14.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.11.0)\n",
            "Installing collected packages: torch, pillow, torchvision\n",
            "  Found existing installation: Pillow 4.0.0\n",
            "    Uninstalling Pillow-4.0.0:\n",
            "      Successfully uninstalled Pillow-4.0.0\n",
            "Successfully installed pillow-5.2.0 torch-0.4.0 torchvision-0.2.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "nHixTUut_LIm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "1ec45c76-26af-47f5-d4ed-b4db73e09804"
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "dtype = torch.float\n",
        "\n",
        "x = torch.tensor([1, 1, 1], dtype=dtype)\n",
        "w = torch.tensor([1.0, 1.0, -1.5], dtype=dtype, requires_grad=True)\n",
        "\n",
        "loss = -torch.dot(x, w).sigmoid().log()\n",
        "loss.backward()\n",
        "print(loss.item())\n",
        "print(w.grad)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.4740769565105438\n",
            "tensor([-0.3775, -0.3775, -0.3775])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "VFzuau5gu4vY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Implementing neural networks with pytorch"
      ]
    },
    {
      "metadata": {
        "id": "STwWdvJCva4G",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Single-layer neural network using automatic differentiation"
      ]
    },
    {
      "metadata": {
        "id": "xOJHDGYKIgsm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "dtype = torch.float\n",
        "\n",
        "# Training data for NAND.\n",
        "x = torch.tensor([[0, 0, 1], [0, 1, 1], [1, 0, 1], [1, 1, 1]], dtype=dtype)\n",
        "y = torch.tensor([[1], [1], [1], [0]], dtype=dtype)\n",
        "w = torch.randn(3, 1, dtype=dtype, requires_grad=True)\n",
        "\n",
        "eta = 0.5\n",
        "for t in range(100):\n",
        "    # y_pred = \\sigma(x \\cdot w)\n",
        "    y_pred = x.mm(w).sigmoid()\n",
        "    ll = y * y_pred + (1 - y) * (1 - y_pred)\n",
        "    loss = -ll.log().sum()      # The loss value.\n",
        "    #print(t, loss.item())\n",
        "    loss.backward()             # Compute the gradients of the loss.\n",
        "\n",
        "    with torch.no_grad():\n",
        "        w -= eta * w.grad       # Update weights using SGD.        \n",
        "        w.grad.zero_()          # Clear the gradients for the next iteration."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FyoS5iP6n7Ru",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "818f72a3-7bb2-4064-c4fc-5324f84218fa"
      },
      "cell_type": "code",
      "source": [
        "w"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-4.2076],\n",
              "        [-4.2069],\n",
              "        [ 6.5032]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "metadata": {
        "id": "Jfp604gFn9Yw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "b535f7a2-f1ce-4cda-9473-39c05c7f43b7"
      },
      "cell_type": "code",
      "source": [
        "x.mm(w).sigmoid()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.9985],\n",
              "        [ 0.9086],\n",
              "        [ 0.9085],\n",
              "        [ 0.1288]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "metadata": {
        "id": "w6eu-AuDvl9J",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Multi-layer neural network using automatic differentiation"
      ]
    },
    {
      "metadata": {
        "id": "ts2RTKVPn_xk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "dtype = torch.float\n",
        "\n",
        "# Training data for XOR.\n",
        "x = torch.tensor([[0, 0, 1], [0, 1, 1], [1, 0, 1], [1, 1, 1]], dtype=dtype)\n",
        "y = torch.tensor([[0], [1], [1], [0]], dtype=dtype)\n",
        "w1 = torch.randn(3, 2, dtype=dtype, requires_grad=True)\n",
        "w2 = torch.randn(2, 1, dtype=dtype, requires_grad=True)\n",
        "b2 = torch.randn(1, 1, dtype=dtype, requires_grad=True)\n",
        "\n",
        "eta = 0.5\n",
        "for t in range(1000):\n",
        "    # y_pred = \\sigma(w_2 \\cdot \\sigma(x \\cdot w_1) + b_2)\n",
        "    y_pred = x.mm(w1).sigmoid().mm(w2).add(b2).sigmoid()\n",
        "    ll = y * y_pred + (1 - y) * (1 - y_pred)\n",
        "    loss = -ll.log().sum()\n",
        "    #print(t, loss.item())\n",
        "    loss.backward()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        # Update weights using SGD.\n",
        "        w1 -= eta * w1.grad\n",
        "        w2 -= eta * w2.grad\n",
        "        b2 -= eta * b2.grad\n",
        "        \n",
        "        # Clear the gradients for the next iteration.\n",
        "        w1.grad.zero_()\n",
        "        w2.grad.zero_()\n",
        "        b2.grad.zero_()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FWgbqAXawEof",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "fe2f55e9-de15-49cb-988e-e5520b3709c0"
      },
      "cell_type": "code",
      "source": [
        "print(w1)\n",
        "print(w2)\n",
        "print(b2)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 6.6428,  6.5513],\n",
            "        [-6.8549, -6.2793],\n",
            "        [-3.5480,  3.1245]])\n",
            "tensor([[ 11.2418],\n",
            "        [-10.7852]])\n",
            "tensor([[ 5.0735]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "yS4bql3foxB5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "f9e4ec70-ecda-47f5-fe66-4ec0014e7fe4"
      },
      "cell_type": "code",
      "source": [
        "x.mm(w1).sigmoid().mm(w2).add(b2).sigmoid()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.0071],\n",
              "        [ 0.9904],\n",
              "        [ 0.9936],\n",
              "        [ 0.0060]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "metadata": {
        "id": "OGS23bDSazMJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Single-layer neural network with high-level NN modules"
      ]
    },
    {
      "metadata": {
        "id": "Jt9eizLFo1iN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "dtype = torch.float\n",
        "\n",
        "# Training data for NAND.\n",
        "x = torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=dtype)\n",
        "y = torch.tensor([[1], [1], [1], [0]], dtype=dtype)\n",
        "                                        \n",
        "# Define a neural network using high-level modules.\n",
        "model = torch.nn.Sequential(\n",
        "    torch.nn.Linear(2, 1, bias=True),   # 2 dims (with bias) -> 1 dim\n",
        ")\n",
        "\n",
        "# Binary corss-entropy loss after sigmoid function.\n",
        "loss_fn = torch.nn.BCEWithLogitsLoss(size_average=False)\n",
        "\n",
        "eta = 0.5\n",
        "for t in range(100):\n",
        "    y_pred = model(x)                   # Make predictions.\n",
        "    loss = loss_fn(y_pred, y)           # Compute the loss.\n",
        "    #print(t, loss.item())\n",
        "    \n",
        "    model.zero_grad()                   # Zero-clear the gradients.\n",
        "    loss.backward()                     # Compute the gradients.\n",
        "        \n",
        "    with torch.no_grad():\n",
        "        for param in model.parameters():\n",
        "            param -= eta * param.grad   # Update the parameters using SGD."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Zq6oqLmIENFa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "4c5503ff-93ad-40bd-e543-ff15a0043607"
      },
      "cell_type": "code",
      "source": [
        "model.state_dict()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('0.weight', tensor([[-4.2608, -4.2610]])),\n",
              "             ('0.bias', tensor([ 6.5830]))])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "metadata": {
        "id": "QdPOdNgO840b",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "c4546a9c-cc99-4474-d108-96c83be9d6ab"
      },
      "cell_type": "code",
      "source": [
        "model(x).sigmoid()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.9986],\n",
              "        [ 0.9107],\n",
              "        [ 0.9107],\n",
              "        [ 0.1258]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "metadata": {
        "id": "KfuoJMeqbClA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Multi-layer neural network with high-level NN modules"
      ]
    },
    {
      "metadata": {
        "id": "D6ss25zA9nPk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "dtype = torch.float\n",
        "\n",
        "# Training data for XOR.\n",
        "x = torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=dtype)\n",
        "y = torch.tensor([[0], [1], [1], [0]], dtype=dtype)\n",
        "                                        \n",
        "# Define a neural network using high-level modules.\n",
        "model = torch.nn.Sequential(\n",
        "    torch.nn.Linear(2, 2, bias=True),   # 2 dims (with bias) -> 2 dims\n",
        "    torch.nn.Sigmoid(),                 # Sigmoid function\n",
        "    torch.nn.Linear(2, 1, bias=True),   # 2 dims (with bias) -> 1 dim\n",
        ")\n",
        "\n",
        "# Binary corss-entropy loss after sigmoid function.\n",
        "loss_fn = torch.nn.BCEWithLogitsLoss(size_average=False)\n",
        "\n",
        "eta = 0.5\n",
        "for t in range(1000):\n",
        "    y_pred = model(x)                   # Make predictions.\n",
        "    loss = loss_fn(y_pred, y)           # Compute the loss.\n",
        "    #print(t, loss.item())\n",
        "    \n",
        "    model.zero_grad()                   # Zero-clear the gradients.\n",
        "    loss.backward()                     # Compute the gradients.\n",
        "        \n",
        "    with torch.no_grad():\n",
        "        for param in model.parameters():\n",
        "            param -= eta * param.grad   # Update the parameters using SGD."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iLFGZWL0--2n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "outputId": "3e9348be-6047-4140-a1fd-92a13121235c"
      },
      "cell_type": "code",
      "source": [
        "model.state_dict()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('0.weight', tensor([[ 5.1500,  5.1430],\n",
              "                      [ 7.0787,  7.0394]])),\n",
              "             ('0.bias', tensor([-7.8774, -3.1887])),\n",
              "             ('2.weight', tensor([[-12.1440,  11.3434]])),\n",
              "             ('2.bias', tensor([-5.2512]))])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "metadata": {
        "id": "1BF6-W_J-82M",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "f787e8f9-ced9-4825-9429-d9db753e487e"
      },
      "cell_type": "code",
      "source": [
        "model(x).sigmoid()"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.0081],\n",
              "        [ 0.9940],\n",
              "        [ 0.9941],\n",
              "        [ 0.0063]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "metadata": {
        "id": "gGK3DJBubb0f",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Single-layer neural network with an optimizer."
      ]
    },
    {
      "metadata": {
        "id": "puqbP4F9bidv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "dtype = torch.float\n",
        "\n",
        "# Training data for NAND.\n",
        "x = torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=dtype)\n",
        "y = torch.tensor([[1], [1], [1], [0]], dtype=dtype)\n",
        "                                        \n",
        "# Define a neural network using high-level modules.\n",
        "model = torch.nn.Sequential(\n",
        "    torch.nn.Linear(2, 1, bias=True),   # 2 dims (with bias) -> 1 dim\n",
        ")\n",
        "\n",
        "# Binary corss-entropy loss after sigmoid function.\n",
        "loss_fn = torch.nn.BCEWithLogitsLoss(size_average=False)\n",
        "\n",
        "# Optimizer based on SGD (change \"SGD\" to \"Adam\" to use Adam)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.5)\n",
        "\n",
        "for t in range(100):\n",
        "    y_pred = model(x)           # Make predictions.\n",
        "    loss = loss_fn(y_pred, y)   # Compute the loss.\n",
        "    #print(t, loss.item())\n",
        "    \n",
        "    optimizer.zero_grad()       # Zero-clear gradients.\n",
        "    loss.backward()             # Compute the gradients.\n",
        "    optimizer.step()            # Update the parameters using the gradients."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RBUX1BUhcDK4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "648a2b78-b10e-4b25-9e19-1a52593f07ec"
      },
      "cell_type": "code",
      "source": [
        "model.state_dict()"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('0.weight', tensor([[-4.2332, -4.2330]])),\n",
              "             ('0.bias', tensor([ 6.5417]))])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "metadata": {
        "id": "nJWcDaPpcKCB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "1ce13e43-eb2f-4dd8-cd75-6e531981237f"
      },
      "cell_type": "code",
      "source": [
        "model(x).sigmoid()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.9986],\n",
              "        [ 0.9096],\n",
              "        [ 0.9096],\n",
              "        [ 0.1274]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "metadata": {
        "id": "scIE8zZWdhLs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Multi-layer neural networks using an optimizer"
      ]
    },
    {
      "metadata": {
        "id": "J1BrWIxkcMfI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "dtype = torch.float\n",
        "\n",
        "# Training data for XOR.\n",
        "x = torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=dtype)\n",
        "y = torch.tensor([[0], [1], [1], [0]], dtype=dtype)\n",
        "                                        \n",
        "# Define a neural network using high-level modules.\n",
        "model = torch.nn.Sequential(\n",
        "    torch.nn.Linear(2, 2, bias=True),   # 2 dims (with bias) -> 2 dims\n",
        "    torch.nn.Sigmoid(),                 # Sigmoid function\n",
        "    torch.nn.Linear(2, 1, bias=True),   # 2 dims (with bias) -> 1 dim\n",
        ")\n",
        "\n",
        "# Binary corss-entropy loss after sigmoid function.\n",
        "loss_fn = torch.nn.BCEWithLogitsLoss(size_average=False)\n",
        "\n",
        "# Optimizer based on SGD (change \"SGD\" to \"Adam\" to use Adam)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.5)\n",
        "\n",
        "for t in range(1000):\n",
        "    y_pred = model(x)           # Make predictions.\n",
        "    loss = loss_fn(y_pred, y)   # Compute the loss.\n",
        "    #print(t, loss.item())\n",
        "    \n",
        "    optimizer.zero_grad()       # Zero-clear gradients.\n",
        "    loss.backward()             # Compute the gradients.\n",
        "    optimizer.step()            # Update the parameters using the gradients."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mp0sNnxhducs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "outputId": "ccf5bace-bd10-45d1-fdb6-3b6665cf8c13"
      },
      "cell_type": "code",
      "source": [
        "model.state_dict()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('0.weight', tensor([[-3.9487,  8.3311],\n",
              "                      [ 4.9970,  8.4306]])),\n",
              "             ('0.bias', tensor([ 2.4715, -1.4012])),\n",
              "             ('2.weight', tensor([[-6.8466,  7.5803]])),\n",
              "             ('2.bias', tensor([-0.7328]))])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "metadata": {
        "id": "AYgEteqydwt2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "90b28bf2-63a6-4130-93b1-77182f8b6fca"
      },
      "cell_type": "code",
      "source": [
        "model(x).sigmoid()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.0039],\n",
              "        [ 0.4986],\n",
              "        [ 0.9954],\n",
              "        [ 0.5020]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    }
  ]
}