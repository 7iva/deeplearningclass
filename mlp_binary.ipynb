{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mlp_binary.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/chokkan/deeplearningclass/blob/master/mlp_binary.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "CX_9BuPB_hA-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Feedforward Neural Networks\n",
        "\n",
        "This Jupyter notebook explains various ways of implementing single-layer and multi-layer neural networks. The implementations are arranged by concrete (explicit) to abstract order so that one can understand the black-boxed processing in deep learning frameworks.\n",
        "\n",
        "In order to focus on understanding the internals of training, this notebook uses a simple and classic example: *threshold logic units*.\n",
        "Supposing $x=0$ as *false* and $x=1$ as *true*, single-layer neural networks can realize logic units such as AND ($\\wedge$), OR ($\\vee$), NOT ($\\lnot$), and NAND ($|$). Multi-layer neural networks can realize logical compounds such as XOR.\n",
        "\n",
        "| $x_1$ | $x_2$ | AND | OR | NAND | XOR |\n",
        "| :---: |:-----:|:---:|:--:|:----:|:---:|\n",
        "| 0 | 0 | 0 | 0 | 1 | 0 |\n",
        "| 0 | 1 | 0 | 1 | 1 | 1 |\n",
        "| 1 | 0 | 0 | 1 | 1 | 1 |\n",
        "| 1 | 1 | 1 | 1 | 0 | 0 |\n"
      ]
    },
    {
      "metadata": {
        "id": "qxdFG8U2Net8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Using numpy"
      ]
    },
    {
      "metadata": {
        "id": "Ea5cM4JEsENr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Single-layer perceptron\n",
        "\n",
        "A single layer perceptron predicts a binary label $\\hat{y} \\in \\{0, 1\\}$ for a given input vector $\\boldsymbol{x} \\in \\mathbb{R}^d$ ($d$ presents the number of dimensions of inputs) by using the following formula,\n",
        "$$\n",
        "\\hat{y} = g(\\boldsymbol{w} \\cdot \\boldsymbol{x} + b) = g(w_1 x_1 + w_2 x_2 + ... + w_d x_d + b)\n",
        "$$\n",
        "\n",
        "Here, $\\boldsymbol{w} \\in \\mathbb{R}^d$ is a weight vector; $b \\in \\mathbb{R}$ is a bias weight; and $g(.)$ denotes a Heaviside step function (we assume $g(0)=0$).\n",
        "\n",
        "Let's train a NAND gate with two inputs ($d = 2$). More specifically, we want to find a weight vector $\\boldsymbol{w}$ and a bias weight $b$ of a single-layer perceptron that realizes the truth table of the NAND gate: $\\{0,1\\}^2 \\to \\{0,1\\}$.\n",
        "\n",
        "| $x_1$ | $x_2$ | $y$  |\n",
        "| :---: |:-----:|:----:|\n",
        "| 0 | 0 | 1|\n",
        "| 0 | 1 | 1|\n",
        "| 1 | 0 | 1|\n",
        "| 1 | 1 | 0|\n",
        "\n",
        "We convert the truth table into a training set consisting of all mappings of the NAND gate,\n",
        "$$\n",
        "\\boldsymbol{x}_1 = (0, 0), y_1 = 1 \\\\\n",
        "\\boldsymbol{x}_2 = (0, 1), y_2 = 1 \\\\\n",
        "\\boldsymbol{x}_3 = (1, 0), y_3 = 1 \\\\\n",
        "\\boldsymbol{x}_4 = (1, 1), y_4 = 0 \\\\\n",
        "$$\n",
        "\n",
        "In order to train a weight vector and bias weight in a unified code, we include a bias term as an additional dimension to inputs. More concretely, we append $1$ to each input,\n",
        "$$\n",
        "\\boldsymbol{x}'_1 = (0, 0, 1), y_1 = 1 \\\\\n",
        "\\boldsymbol{x}'_2 = (0, 1, 1), y_2 = 1 \\\\\n",
        "\\boldsymbol{x}'_3 = (1, 0, 1), y_3 = 1 \\\\\n",
        "\\boldsymbol{x}'_4 = (1, 1, 1), y_4 = 0 \\\\\n",
        "$$\n",
        "\n",
        "Then, the formula of the single-layer perceptron becomes,\n",
        "$$\n",
        "\\hat{y} = g((w_1, w_2, w_3) \\cdot \\boldsymbol{x}') = g(w_1 x_1 + w_2 x_2 + w_3)\n",
        "$$\n",
        "In other words, $w_1$ and $w_2$ present weights for $x_1$ and $x_2$, respectively, and $w_3$ does a bias weight.\n",
        "\n",
        "The code below implements Rosenblatt's perceptron algorithm with a fixed number of iterations (100 times). We use a constant learning rate 0.5 for simplicity.\n"
      ]
    },
    {
      "metadata": {
        "id": "2ygoUjQYrPoj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Training data for NAND.\n",
        "x = np.array([\n",
        "    [0, 0, 1], [0, 1, 1], [1, 0, 1], [1, 1, 1]\n",
        "    ])\n",
        "y = np.array([0, 0, 0, 1])\n",
        "w = np.array([0.0, 0.0, 0.0])\n",
        "\n",
        "eta = 0.5\n",
        "for t in range(100):\n",
        "    for i in range(len(y)):\n",
        "        y_pred = np.heaviside(np.dot(x[i], w), 0)\n",
        "        w += (y[i] - y_pred) * eta * x[i]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TYoeshu2rXdK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "10158ce2-a1ea-441a-8193-55679e0e45c2"
      },
      "cell_type": "code",
      "source": [
        "w"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 1. ,  0.5, -1. ])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "metadata": {
        "id": "hOFgUFojraFA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "09ef06b1-66a8-4c33-bb8a-d4e3ec843072"
      },
      "cell_type": "code",
      "source": [
        "np.heaviside(np.dot(x, w), 0)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 0., 0., 1.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "metadata": {
        "id": "bl_ZAEguNzgU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Single-layer perceptron with mini-batch\n",
        "\n",
        "It is desireable to reduce the execusion run by the Python interpreter, which is relatively slow. The common technique to speed up a machine-learning code written in Python is to to execute computations within the matrix library (e.g., numpy).\n",
        "\n",
        "The single-layer perceptron makes predictions for four inputs,\n",
        "$$\n",
        "\\hat{y}_1 = g(\\boldsymbol{x}_1 \\cdot \\boldsymbol{w}) \\\\\n",
        "\\hat{y}_2 = g(\\boldsymbol{x}_2 \\cdot \\boldsymbol{w}) \\\\\n",
        "\\hat{y}_3 = g(\\boldsymbol{x}_3 \\cdot \\boldsymbol{w}) \\\\\n",
        "\\hat{y}_4 = g(\\boldsymbol{x}_4 \\cdot \\boldsymbol{w}) \\\\\n",
        "$$\n",
        "\n",
        "Here, we define $\\hat{Y} \\in \\mathbb{R}^{4 \\times 1}$ and $X \\in \\mathbb{R}^{4 \\times d}$ as,\n",
        "$$\n",
        "\\hat{Y} = \\begin{pmatrix} \n",
        "  \\hat{y}_1 \\\\ \n",
        "  \\hat{y}_2 \\\\ \n",
        "  \\hat{y}_3 \\\\ \n",
        "  \\hat{y}_4 \\\\ \n",
        "\\end{pmatrix},\n",
        "X = \\begin{pmatrix} \n",
        "  \\boldsymbol{x}_1 \\\\ \n",
        "  \\boldsymbol{x}_2 \\\\ \n",
        "  \\boldsymbol{x}_3 \\\\ \n",
        "  \\boldsymbol{x}_4 \\\\ \n",
        "\\end{pmatrix}\n",
        "$$\n",
        "\n",
        "Then, we can write the four predictions in one dot-product computation,\n",
        "$$\n",
        "\\hat{Y} = X \\cdot \\boldsymbol{w}\n",
        "$$\n",
        "\n",
        "The code below implements this idea. The function `np.heaviside()` yields a vector corresponding to the four predictions, applying the step function for every element of the argument.\n",
        "\n",
        "This technique is frequently used in mini-batch training."
      ]
    },
    {
      "metadata": {
        "id": "2fK-_WimtPwb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Training data for NAND.\n",
        "x = np.array([\n",
        "    [0, 0, 1], [0, 1, 1], [1, 0, 1], [1, 1, 1]\n",
        "    ])\n",
        "y = np.array([1, 1, 1, 0])\n",
        "w = np.array([0.0, 0.0, 0.0])\n",
        "\n",
        "eta = 0.5\n",
        "for t in range(100):\n",
        "    y_pred = np.heaviside(np.dot(x, w), 0)\n",
        "    w += np.dot((y - y_pred), x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_x4p1BldtQ-K",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "8f48dd40-083b-4a18-dcee-69e5862270c2"
      },
      "cell_type": "code",
      "source": [
        "w"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-1., -1.,  2.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "metadata": {
        "id": "E-P2RpWrtVyf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "80bfbe45-5a55-4858-8104-65c39d6dfd77"
      },
      "cell_type": "code",
      "source": [
        "np.heaviside(np.dot(x, w), 0)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1., 1., 1., 0.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "metadata": {
        "id": "nkxBcCSTtDvm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Stochastic gradient descent (SGD) with mini-batch"
      ]
    },
    {
      "metadata": {
        "id": "bltpfNRctjV5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def sigmoid(v):\n",
        "    return 1.0 / (1 + np.exp(-v))\n",
        "\n",
        "# Training data for NAND.\n",
        "x = np.array([\n",
        "    [0, 0, 1], [0, 1, 1], [1, 0, 1], [1, 1, 1]\n",
        "    ])\n",
        "y = np.array([1, 1, 1, 0])\n",
        "w = np.array([0.0, 0.0, 0.0])\n",
        "\n",
        "eta = 0.5\n",
        "for t in range(100):\n",
        "    y_pred = sigmoid(np.dot(x, w))\n",
        "    w -= np.dot((y_pred - y), x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9bASDMfhtm-I",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d3ebf3eb-090f-4c56-fd59-7470c478073e"
      },
      "cell_type": "code",
      "source": [
        "w"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-5.59504346, -5.59504346,  8.57206068])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "metadata": {
        "id": "-69r0c4KtqlW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "5353fed4-25c8-4bd3-e592-6f85be20d9df"
      },
      "cell_type": "code",
      "source": [
        "sigmoid(np.dot(x, w))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.99981071, 0.95152498, 0.95152498, 0.06798725])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "metadata": {
        "id": "ELUeNFRFuJv3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Automatic differentiation"
      ]
    },
    {
      "metadata": {
        "id": "aB0DwVOyuXP_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Using autograd\n",
        "\n",
        "Installing [autograd](https://github.com/HIPS/autograd) (do this once)."
      ]
    },
    {
      "metadata": {
        "id": "kpt7DJ7a-qov",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "outputId": "0d009c73-ef01-4e6c-a208-892f6a60d921"
      },
      "cell_type": "code",
      "source": [
        "!pip install autograd"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting autograd\n",
            "  Downloading https://files.pythonhosted.org/packages/08/7a/1ccee2a929d806ba3dbe632a196ad6a3f1423d6e261ae887e5fef2011420/autograd-1.2.tar.gz\n",
            "Requirement already satisfied: numpy>=1.12 in /usr/local/lib/python3.6/dist-packages (from autograd) (1.14.5)\n",
            "Requirement already satisfied: future>=0.15.2 in /usr/local/lib/python3.6/dist-packages (from autograd) (0.16.0)\n",
            "Building wheels for collected packages: autograd\n",
            "  Running setup.py bdist_wheel for autograd ... \u001b[?25l-\b \b\\\b \bdone\n",
            "\u001b[?25h  Stored in directory: /content/.cache/pip/wheels/72/6f/c2/40f130cca2c91f31d354bf72de282922479c09ce0b7853c4c5\n",
            "Successfully built autograd\n",
            "Installing collected packages: autograd\n",
            "Successfully installed autograd-1.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "DyC9iOFO-1cd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "e37bc42a-ce49-4436-e09d-527d256fa2b7"
      },
      "cell_type": "code",
      "source": [
        "import autograd\n",
        "import autograd.numpy as np\n",
        "\n",
        "def loss(w, x):\n",
        "    return -np.log(1.0 / (1 + np.exp(-np.dot(x, w))))\n",
        "\n",
        "x = np.array([1, 1, 1])\n",
        "w = np.array([1.0, 1.0, -1.5])\n",
        "\n",
        "grad_loss = autograd.grad(loss)\n",
        "print(loss(w, x))\n",
        "print(grad_loss(w, x))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.47407698418010663\n",
            "[-0.37754067 -0.37754067 -0.37754067]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "59D2aARTuQDL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Using pytorch"
      ]
    },
    {
      "metadata": {
        "id": "VOoZXvXP-6b2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "outputId": "5253d391-8375-4fd9-b0b7-df255328ccfe"
      },
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (0.4.0)\r\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.2.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.14.5)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (5.2.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.11.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "nHixTUut_LIm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "98b7ce9a-4590-4dc2-d855-d5eb26863757"
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "dtype = torch.float\n",
        "\n",
        "x = torch.tensor([1, 1, 1], dtype=dtype)\n",
        "w = torch.tensor([1.0, 1.0, -1.5], dtype=dtype, requires_grad=True)\n",
        "\n",
        "loss = -torch.dot(x, w).sigmoid().log()\n",
        "loss.backward()\n",
        "print(loss.item())\n",
        "print(w.grad)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.4740769565105438\n",
            "tensor([-0.3775, -0.3775, -0.3775])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "VFzuau5gu4vY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Implementing neural networks with pytorch"
      ]
    },
    {
      "metadata": {
        "id": "STwWdvJCva4G",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Single-layer neural network using automatic differentiation"
      ]
    },
    {
      "metadata": {
        "id": "xOJHDGYKIgsm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "dtype = torch.float\n",
        "\n",
        "# Training data for NAND.\n",
        "x = torch.tensor([[0, 0, 1], [0, 1, 1], [1, 0, 1], [1, 1, 1]], dtype=dtype)\n",
        "y = torch.tensor([[1], [1], [1], [0]], dtype=dtype)\n",
        "w = torch.randn(3, 1, dtype=dtype, requires_grad=True)\n",
        "\n",
        "eta = 0.5\n",
        "for t in range(100):\n",
        "    # y_pred = \\sigma(x \\cdot w)\n",
        "    y_pred = x.mm(w).sigmoid()\n",
        "    ll = y * y_pred + (1 - y) * (1 - y_pred)\n",
        "    loss = -ll.log().sum()      # The loss value.\n",
        "    #print(t, loss.item())\n",
        "    loss.backward()             # Compute the gradients of the loss.\n",
        "\n",
        "    with torch.no_grad():\n",
        "        w -= eta * w.grad       # Update weights using SGD.        \n",
        "        w.grad.zero_()          # Clear the gradients for the next iteration."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FyoS5iP6n7Ru",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "807af88f-be7e-45d0-b28b-cc4846f0afda"
      },
      "cell_type": "code",
      "source": [
        "w"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-4.2937],\n",
              "        [-4.2952],\n",
              "        [ 6.6328]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "metadata": {
        "id": "Jfp604gFn9Yw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "066c1d31-8005-49b8-b759-cdbfb5609c47"
      },
      "cell_type": "code",
      "source": [
        "x.mm(w).sigmoid()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.9987],\n",
              "        [ 0.9120],\n",
              "        [ 0.9121],\n",
              "        [ 0.1239]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "metadata": {
        "id": "w6eu-AuDvl9J",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Multi-layer neural network using automatic differentiation"
      ]
    },
    {
      "metadata": {
        "id": "ts2RTKVPn_xk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "dtype = torch.float\n",
        "\n",
        "# Training data for XOR.\n",
        "x = torch.tensor([[0, 0, 1], [0, 1, 1], [1, 0, 1], [1, 1, 1]], dtype=dtype)\n",
        "y = torch.tensor([[0], [1], [1], [0]], dtype=dtype)\n",
        "w1 = torch.randn(3, 2, dtype=dtype, requires_grad=True)\n",
        "w2 = torch.randn(2, 1, dtype=dtype, requires_grad=True)\n",
        "b2 = torch.randn(1, 1, dtype=dtype, requires_grad=True)\n",
        "\n",
        "eta = 0.5\n",
        "for t in range(1000):\n",
        "    # y_pred = \\sigma(w_2 \\cdot \\sigma(x \\cdot w_1) + b_2)\n",
        "    y_pred = x.mm(w1).sigmoid().mm(w2).add(b2).sigmoid()\n",
        "    ll = y * y_pred + (1 - y) * (1 - y_pred)\n",
        "    loss = -ll.log().sum()\n",
        "    #print(t, loss.item())\n",
        "    loss.backward()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        # Update weights using SGD.\n",
        "        w1 -= eta * w1.grad\n",
        "        w2 -= eta * w2.grad\n",
        "        b2 -= eta * b2.grad\n",
        "        \n",
        "        # Clear the gradients for the next iteration.\n",
        "        w1.grad.zero_()\n",
        "        w2.grad.zero_()\n",
        "        b2.grad.zero_()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FWgbqAXawEof",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "9ad1a810-37bb-46eb-9275-ac1e1093d03b"
      },
      "cell_type": "code",
      "source": [
        "print(w1)\n",
        "print(w2)\n",
        "print(b2)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 6.3815,  7.8206],\n",
            "        [-6.0017, -7.5960],\n",
            "        [ 2.9202, -4.3392]])\n",
            "tensor([[-10.4501],\n",
            "        [ 10.8341]])\n",
            "tensor([[ 4.9265]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "yS4bql3foxB5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "c1824fd7-e801-42fb-a89f-34ea8dd3f07f"
      },
      "cell_type": "code",
      "source": [
        "x.mm(w1).sigmoid().mm(w2).add(b2).sigmoid()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.0078],\n",
              "        [ 0.9887],\n",
              "        [ 0.9932],\n",
              "        [ 0.0068]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "metadata": {
        "id": "OGS23bDSazMJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Single-layer neural network with high-level NN modules"
      ]
    },
    {
      "metadata": {
        "id": "Jt9eizLFo1iN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "dtype = torch.float\n",
        "\n",
        "# Training data for NAND.\n",
        "x = torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=dtype)\n",
        "y = torch.tensor([[1], [1], [1], [0]], dtype=dtype)\n",
        "                                        \n",
        "# Define a neural network using high-level modules.\n",
        "model = torch.nn.Sequential(\n",
        "    torch.nn.Linear(2, 1, bias=True),   # 2 dims (with bias) -> 1 dim\n",
        ")\n",
        "\n",
        "# Binary corss-entropy loss after sigmoid function.\n",
        "loss_fn = torch.nn.BCEWithLogitsLoss(size_average=False)\n",
        "\n",
        "eta = 0.5\n",
        "for t in range(100):\n",
        "    y_pred = model(x)                   # Make predictions.\n",
        "    loss = loss_fn(y_pred, y)           # Compute the loss.\n",
        "    #print(t, loss.item())\n",
        "    \n",
        "    model.zero_grad()                   # Zero-clear the gradients.\n",
        "    loss.backward()                     # Compute the gradients.\n",
        "        \n",
        "    with torch.no_grad():\n",
        "        for param in model.parameters():\n",
        "            param -= eta * param.grad   # Update the parameters using SGD."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Zq6oqLmIENFa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "df8a807a-a040-402a-bdf3-a0f87aab5bc8"
      },
      "cell_type": "code",
      "source": [
        "model.state_dict()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('0.weight', tensor([[-4.2644, -4.2640]])),\n",
              "             ('0.bias', tensor([ 6.5879]))])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "metadata": {
        "id": "QdPOdNgO840b",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "993db76b-c74e-47f4-fab7-777542647323"
      },
      "cell_type": "code",
      "source": [
        "model(x).sigmoid()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.9986],\n",
              "        [ 0.9108],\n",
              "        [ 0.9108],\n",
              "        [ 0.1256]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "metadata": {
        "id": "KfuoJMeqbClA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Multi-layer neural network with high-level NN modules"
      ]
    },
    {
      "metadata": {
        "id": "D6ss25zA9nPk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "dtype = torch.float\n",
        "\n",
        "# Training data for XOR.\n",
        "x = torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=dtype)\n",
        "y = torch.tensor([[0], [1], [1], [0]], dtype=dtype)\n",
        "                                        \n",
        "# Define a neural network using high-level modules.\n",
        "model = torch.nn.Sequential(\n",
        "    torch.nn.Linear(2, 2, bias=True),   # 2 dims (with bias) -> 2 dims\n",
        "    torch.nn.Sigmoid(),                 # Sigmoid function\n",
        "    torch.nn.Linear(2, 1, bias=True),   # 2 dims (with bias) -> 1 dim\n",
        ")\n",
        "\n",
        "# Binary corss-entropy loss after sigmoid function.\n",
        "loss_fn = torch.nn.BCEWithLogitsLoss(size_average=False)\n",
        "\n",
        "eta = 0.5\n",
        "for t in range(1000):\n",
        "    y_pred = model(x)                   # Make predictions.\n",
        "    loss = loss_fn(y_pred, y)           # Compute the loss.\n",
        "    #print(t, loss.item())\n",
        "    \n",
        "    model.zero_grad()                   # Zero-clear the gradients.\n",
        "    loss.backward()                     # Compute the gradients.\n",
        "        \n",
        "    with torch.no_grad():\n",
        "        for param in model.parameters():\n",
        "            param -= eta * param.grad   # Update the parameters using SGD."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iLFGZWL0--2n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "outputId": "8a824d6a-46c9-497b-b494-66847d72007a"
      },
      "cell_type": "code",
      "source": [
        "model.state_dict()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('0.weight', tensor([[-6.8318, -6.8475],\n",
              "                      [-5.3508, -5.3534]])),\n",
              "             ('0.bias', tensor([ 2.8625,  7.9754])),\n",
              "             ('2.weight', tensor([[-11.7905,  11.3349]])),\n",
              "             ('2.bias', tensor([-5.3092]))])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "metadata": {
        "id": "1BF6-W_J-82M",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "9a2946ed-49a6-4716-a54d-0338f8c5bee2"
      },
      "cell_type": "code",
      "source": [
        "model(x).sigmoid()"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.0059],\n",
              "        [ 0.9936],\n",
              "        [ 0.9936],\n",
              "        [ 0.0098]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "metadata": {
        "id": "gGK3DJBubb0f",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Single-layer neural network with an optimizer."
      ]
    },
    {
      "metadata": {
        "id": "puqbP4F9bidv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "dtype = torch.float\n",
        "\n",
        "# Training data for NAND.\n",
        "x = torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=dtype)\n",
        "y = torch.tensor([[1], [1], [1], [0]], dtype=dtype)\n",
        "                                        \n",
        "# Define a neural network using high-level modules.\n",
        "model = torch.nn.Sequential(\n",
        "    torch.nn.Linear(2, 1, bias=True),   # 2 dims (with bias) -> 1 dim\n",
        ")\n",
        "\n",
        "# Binary corss-entropy loss after sigmoid function.\n",
        "loss_fn = torch.nn.BCEWithLogitsLoss(size_average=False)\n",
        "\n",
        "# Optimizer based on SGD (change \"SGD\" to \"Adam\" to use Adam)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.5)\n",
        "\n",
        "for t in range(100):\n",
        "    y_pred = model(x)           # Make predictions.\n",
        "    loss = loss_fn(y_pred, y)   # Compute the loss.\n",
        "    #print(t, loss.item())\n",
        "    \n",
        "    optimizer.zero_grad()       # Zero-clear gradients.\n",
        "    loss.backward()             # Compute the gradients.\n",
        "    optimizer.step()            # Update the parameters using the gradients."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RBUX1BUhcDK4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "52114453-9462-4b65-b902-6a7a9597adce"
      },
      "cell_type": "code",
      "source": [
        "model.state_dict()"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('0.weight', tensor([[-4.2736, -4.2739]])),\n",
              "             ('0.bias', tensor([ 6.6021]))])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "metadata": {
        "id": "nJWcDaPpcKCB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "4c6cd092-0da8-41dc-ac43-ae13250090bc"
      },
      "cell_type": "code",
      "source": [
        "model(x).sigmoid()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.9986],\n",
              "        [ 0.9112],\n",
              "        [ 0.9112],\n",
              "        [ 0.1251]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "metadata": {
        "id": "scIE8zZWdhLs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Multi-layer neural networks using an optimizer"
      ]
    },
    {
      "metadata": {
        "id": "J1BrWIxkcMfI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "dtype = torch.float\n",
        "\n",
        "# Training data for XOR.\n",
        "x = torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=dtype)\n",
        "y = torch.tensor([[0], [1], [1], [0]], dtype=dtype)\n",
        "                                        \n",
        "# Define a neural network using high-level modules.\n",
        "model = torch.nn.Sequential(\n",
        "    torch.nn.Linear(2, 2, bias=True),   # 2 dims (with bias) -> 2 dims\n",
        "    torch.nn.Sigmoid(),                 # Sigmoid function\n",
        "    torch.nn.Linear(2, 1, bias=True),   # 2 dims (with bias) -> 1 dim\n",
        ")\n",
        "\n",
        "# Binary corss-entropy loss after sigmoid function.\n",
        "loss_fn = torch.nn.BCEWithLogitsLoss(size_average=False)\n",
        "\n",
        "# Optimizer based on SGD (change \"SGD\" to \"Adam\" to use Adam)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.5)\n",
        "\n",
        "for t in range(1000):\n",
        "    y_pred = model(x)           # Make predictions.\n",
        "    loss = loss_fn(y_pred, y)   # Compute the loss.\n",
        "    #print(t, loss.item())\n",
        "    \n",
        "    optimizer.zero_grad()       # Zero-clear gradients.\n",
        "    loss.backward()             # Compute the gradients.\n",
        "    optimizer.step()            # Update the parameters using the gradients."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mp0sNnxhducs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "outputId": "d9e96d4b-781f-4366-f754-86463227c564"
      },
      "cell_type": "code",
      "source": [
        "model.state_dict()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('0.weight', tensor([[ 6.1945, -6.4531],\n",
              "                      [ 6.8025, -6.5897]])),\n",
              "             ('0.bias', tensor([-3.3006,  3.3070])),\n",
              "             ('2.weight', tensor([[ 11.4259, -10.8381]])),\n",
              "             ('2.bias', tensor([ 5.0708]))])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "metadata": {
        "id": "AYgEteqydwt2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "1ca1a292-d4af-42c3-d56c-e455c3ddc17c"
      },
      "cell_type": "code",
      "source": [
        "model(x).sigmoid()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.0068],\n",
              "        [ 0.9908],\n",
              "        [ 0.9937],\n",
              "        [ 0.0058]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "metadata": {
        "id": "klKcvlpVkpk9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "dtype = torch.float\n",
        "\n",
        "# Training data for NAND.\n",
        "x = torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=dtype)\n",
        "y = torch.tensor([[1], [1], [1], [0]], dtype=dtype)\n",
        "                                        \n",
        "# Define a neural network model.\n",
        "class SingleLayerNN(torch.nn.Module):\n",
        "    def __init__(self, d_in, d_out):\n",
        "        super(SingleLayerNN, self).__init__()\n",
        "        self.linear1 = torch.nn.Linear(d_in, d_out, bias=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear1(x)\n",
        "\n",
        "model = SingleLayerNN(2, 1)\n",
        "\n",
        "# Binary corss-entropy loss after sigmoid function.\n",
        "loss_fn = torch.nn.BCEWithLogitsLoss(size_average=False)\n",
        "\n",
        "# Optimizer based on SGD (change \"SGD\" to \"Adam\" to use Adam)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.5)\n",
        "\n",
        "for t in range(100):\n",
        "    y_pred = model(x)           # Make predictions.\n",
        "    loss = loss_fn(y_pred, y)   # Compute the loss.\n",
        "    #print(t, loss.item())\n",
        "    \n",
        "    optimizer.zero_grad()       # Zero-clear gradients.\n",
        "    loss.backward()             # Compute the gradients.\n",
        "    optimizer.step()            # Update the parameters using the gradients."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ExxVfDnB5vPW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "b3df9fb8-cb72-47a4-c0ed-b772352ef9f6"
      },
      "cell_type": "code",
      "source": [
        "model.state_dict()"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('linear1.weight', tensor([[-4.2345, -4.2340]])),\n",
              "             ('linear1.bias', tensor([ 6.5433]))])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "metadata": {
        "id": "xKczD7tZ518W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "e6b8ce91-a73b-48d2-aa84-b4b2814bcbe4"
      },
      "cell_type": "code",
      "source": [
        "model(x).sigmoid()"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.9986],\n",
              "        [ 0.9097],\n",
              "        [ 0.9096],\n",
              "        [ 0.1273]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "metadata": {
        "id": "gSpf1qft53-O",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "dtype = torch.float\n",
        "\n",
        "# Training data for XOR.\n",
        "x = torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=dtype)\n",
        "y = torch.tensor([[0], [1], [1], [0]], dtype=dtype)\n",
        "                                        \n",
        "# Define a neural network model.\n",
        "class ThreeLayerNN(torch.nn.Module):\n",
        "    def __init__(self, d_in, d_hidden, d_out):\n",
        "        super(ThreeLayerNN, self).__init__()\n",
        "        self.linear1 = torch.nn.Linear(d_in, d_hidden, bias=True)\n",
        "        self.linear2 = torch.nn.Linear(d_hidden, d_out, bias=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear2(self.linear1(x).sigmoid())\n",
        "\n",
        "model = ThreeLayerNN(2, 2, 1)\n",
        "\n",
        "# Binary corss-entropy loss after sigmoid function.\n",
        "loss_fn = torch.nn.BCEWithLogitsLoss(size_average=False)\n",
        "\n",
        "# Optimizer based on SGD (change \"SGD\" to \"Adam\" to use Adam)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.5)\n",
        "\n",
        "for t in range(1000):\n",
        "    y_pred = model(x)           # Make predictions.\n",
        "    loss = loss_fn(y_pred, y)   # Compute the loss.\n",
        "    #print(t, loss.item())\n",
        "    \n",
        "    optimizer.zero_grad()       # Zero-clear gradients.\n",
        "    loss.backward()             # Compute the gradients.\n",
        "    optimizer.step()            # Update the parameters using the gradients."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "k6LLbCf0684G",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "outputId": "0e3c278a-6866-4b34-a804-ae3de3c876f6"
      },
      "cell_type": "code",
      "source": [
        "model.state_dict()"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('linear1.weight', tensor([[ 5.8336, -6.1471],\n",
              "                      [-6.6025,  6.4364]])),\n",
              "             ('linear1.bias', tensor([-3.2014, -3.5689])),\n",
              "             ('linear2.weight', tensor([[ 10.1559,  10.0568]])),\n",
              "             ('linear2.bias', tensor([-4.9504]))])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "metadata": {
        "id": "iPhm3J4R61S2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "af47d8a5-c1e7-4b7a-dd6e-66bd30c19917"
      },
      "cell_type": "code",
      "source": [
        "model(x).sigmoid()"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.0137],\n",
              "        [ 0.9897],\n",
              "        [ 0.9893],\n",
              "        [ 0.0119]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "metadata": {
        "id": "OeOa6mt5Fgw4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}